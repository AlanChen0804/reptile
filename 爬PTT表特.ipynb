{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/27\n",
      "昨天有 0 篇文章\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "import datetime\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from apscheduler.schedulers.background import BackgroundScheduler\n",
    "\n",
    "PTT_url = \"https://www.ptt.cc\"\n",
    "\n",
    "def job():\n",
    "    print(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    \n",
    "\n",
    "def get_webPage(url):\n",
    "    res = requests.get(url,cookies = {'over18': '1'})\n",
    "    if res.status_code !=200:\n",
    "        print(\"Invalid URL\",res.url)\n",
    "        return none\n",
    "    else:\n",
    "        return res.text\n",
    "def get_articles(page,date):\n",
    "    soup = BeautifulSoup(page,'html5lib')\n",
    "    \n",
    "    #上一頁連結位置\n",
    "    prevURL = soup.select('.btn-group-paging a')[1]['href']\n",
    "    \n",
    "    #取得文章清單\n",
    "    articles = []\n",
    "    divs = soup.select('.r-ent')\n",
    "    for article in divs:\n",
    "        if article.find('div','date').text.strip() == yesterday:\n",
    "            #取得推文數\n",
    "            pushCount = 0\n",
    "            pushString = article.find('div','nrec').text\n",
    "            if pushString:\n",
    "                try:\n",
    "                    pushCount = int(pushString) #將字串轉換成數字\n",
    "                except ValueError:\n",
    "                    if pushString == \"爆\":\n",
    "                        pushCount = 99\n",
    "                    elif pushString.startswith('X'):\n",
    "                        pushCount = -10\n",
    "            if article.find('a'):\n",
    "                title = article.find('a').text #取得文章標頭\n",
    "                href = article.find('a')['href'] #取得文章連結\n",
    "                author = article.find('div','author').text #取得作者名\n",
    "                articles.append({\n",
    "                    'title':title,\n",
    "                    'href':href,\n",
    "                    'pushCount':pushCount,\n",
    "                    'author':author\n",
    "                })\n",
    "    return articles, prevURL\n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    # BlockingScheduler\n",
    "    scheduler = BackgroundScheduler()\n",
    "    scheduler.add_job(job, 'cron',  hour=0, minute=0) #cron: 在特定時間週期性地觸發\n",
    "    scheduler.start()\n",
    "    \n",
    "    \n",
    "    allArticles = []\n",
    "    # 取得表特版頁面\n",
    "    currentPage = get_webPage(PTT_url+'/bbs/Beauty/index.html')\n",
    "    # 取得電腦端時間資料 - 再減去一天 = 昨天\n",
    "    yesterdayRoot = datetime.date.today() - datetime.timedelta(days=1)\n",
    "    \n",
    "    # 更新為 PTT 時間格式，並去掉開頭的'0' \n",
    "    yesterday = yesterdayRoot.strftime(\"%m/%d\").lstrip('0')\n",
    "    #print(yesterday)\n",
    "    articles, prevURL = get_articles(currentPage,yesterday)\n",
    "    #當有符合日期的文章回傳時，搜尋上一頁是否有文章\n",
    "    while articles:\n",
    "        allArticles += articles\n",
    "        currentPage = get_webPage(PTT_url+prevURL)\n",
    "        articles, prevURL = get_articles(currentPage,yesterday)\n",
    "           \n",
    "        \n",
    "    #擷取文章總覽\n",
    "    print('昨天有', len(allArticles), '篇文章')\n",
    "    #threshold = 40 #定義熱門文章門檻\n",
    "    #print('熱門文章(> %d 推):' % (threshold))\n",
    "    for article in allArticles:\n",
    "            if int(article['pushCount']) > threshold:\n",
    "                print(article['title'], PTT_url + article['href'], '推文數:',  article['pushCount'], '作者:', article['author'])\n",
    "                # 再次進入文章，進行圖片爬取\n",
    "                url = PTT_url+article['href']\n",
    "                newRequest = get_webPage(url)\n",
    "                soup = BeautifulSoup(newRequest,'html5lib')\n",
    "                # 找尋符合的 img 圖片網址\n",
    "                imgLinks = soup.findAll('a',{'href':re.compile('https:\\/\\/(imgur|i\\.imgur)\\.com\\/.*.jpg$')})\n",
    "                # 依照文章標題建立資料夾\n",
    "                folderName = article['title'].strip() # 去除多餘空格\n",
    "                os.makedirs(folderName)\n",
    "                \n",
    "                if len(imgLinks)>0:\n",
    "                   try:\n",
    "                        for imgLink in imgLinks:\n",
    "                            print (imgLink['href'])\n",
    "                            fileName = imgLink['href'].split(\"/\")[-1]\n",
    "                            urllib.request.urlretrieve(imgLink['href'], os.path.join(folderName, fileName))\n",
    "                   except Exception as e:\n",
    "                       print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
